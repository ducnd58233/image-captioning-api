{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7941/3772518769.py:7: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd  # for lookup in annotation file\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "import sys\n",
    "import datetime\n",
    "import time\n",
    "import pandas as pd  # for lookup in annotation file\n",
    "import spacy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as T\n",
    "from torch.nn.utils.rnn import pad_sequence  # pad batch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image  # Load img\n",
    "from nltk.translate.bleu_score import corpus_bleu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "def is_today(date_str):\n",
    "    today = datetime.today().strftime('%Y-%m-%d')\n",
    "    return date_str == today\n",
    "\n",
    "def get_newest_folder_name(directory, pattern='*-*-*'):\n",
    "    folders = glob.glob(os.path.join(directory, pattern), recursive=True)\n",
    "    \n",
    "    newest_folder = max(folders, key=os.path.getmtime)\n",
    "    return newest_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)\n",
    "NUM_WORKERS = 0\n",
    "\n",
    "INPUT_SIZE = (384, 384)\n",
    "EMBED_SIZE = 256\n",
    "HIDDEN_SIZE = 256\n",
    "BATCH_SIZE = 16\n",
    "BASE_LR = 0.1\n",
    "MAX_LR = 1.0\n",
    "MAX_EPOCHS = 200\n",
    "SAVE_INTERVAL = 10\n",
    "\n",
    "EARLY_STOP_THRESHOLD = 20\n",
    "N_SPLITS = 2\n",
    "\n",
    "CHECKPOINT = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jiggle/personal/mle/project/project-1/notebooks/data/2024-02-06/images\n",
      "/home/jiggle/personal/mle/project/project-1/notebooks/data/2024-02-06/captions.csv\n",
      "/home/jiggle/personal/mle/project/project-1/models\n"
     ]
    }
   ],
   "source": [
    "root_dir = f'{os.getcwd()}/data'\n",
    "\n",
    "FOLDER_PATH = get_newest_folder_name(root_dir)\n",
    "\n",
    "IMAGES_DIR = os.path.join(\n",
    "    FOLDER_PATH,\n",
    "    'images'\n",
    ")\n",
    "\n",
    "CAPTIONS_DIR = os.path.join(\n",
    "    FOLDER_PATH,\n",
    "    'captions.csv'\n",
    ")\n",
    "\n",
    "OUTPUT_DIR = f'{os.path.dirname(os.getcwd())}/models'\n",
    "\n",
    "print(IMAGES_DIR)\n",
    "print(CAPTIONS_DIR)\n",
    "print(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Free GPU Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pynvml import nvmlInit, nvmlDeviceGetHandleByIndex, nvmlDeviceGetMemoryInfo\n",
    "from numba import cuda\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:1024\"\n",
    "\n",
    "def free_gpu_cache(*args):\n",
    "    gc.collect()\n",
    "    for param in args:\n",
    "        del param\n",
    "        \n",
    "def free_cuda_cache():\n",
    "    if torch.cuda.is_available():\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "#         cuda.select_device(0)\n",
    "#         cuda.close()\n",
    "#         cuda.select_device(0)\n",
    "\n",
    "\n",
    "def wait_until_enough_gpu_memory(min_memory_available=2 * 1024 * 1024 * 1024, max_retries=10, sleep_time=5): #2GB\n",
    "    nvmlInit()\n",
    "    handle = nvmlDeviceGetHandleByIndex(torch.cuda.current_device())\n",
    "\n",
    "    for _ in range(max_retries):\n",
    "        free_cuda_cache()\n",
    "        info = nvmlDeviceGetMemoryInfo(handle)\n",
    "        if info.free >= min_memory_available:\n",
    "            break\n",
    "        print(f\"Waiting for {min_memory_available} bytes of free GPU memory. Retrying in {sleep_time} seconds...\")\n",
    "        time.sleep(sleep_time)\n",
    "    else:\n",
    "        raise RuntimeError(f\"Failed to acquire {min_memory_available} bytes of free GPU memory after {max_retries} retries.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Custom Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "class LogCustomFormatter(logging.Formatter):\n",
    "    grey = \"\\x1b[0;37m\"\n",
    "    green = \"\\x1b[1;32m\"\n",
    "    bold_red = \"\\x1b[31;1m\"\n",
    "    yellow = \"\\x1b[1;33m\"\n",
    "    red = \"\\x1b[1;31m\"\n",
    "    purple = \"\\x1b[1;35m\"\n",
    "    blue = \"\\x1b[1;34m\"\n",
    "    light_blue = \"\\x1b[1;36m\"\n",
    "    reset = \"\\x1b[0m\"\n",
    "    blink_red = \"\\x1b[5m\\x1b[1;31m\"\n",
    "\n",
    "    format = '%(asctime)s - %(name)s - %(levelname)s - %(message)s (%(filename)s:%(lineno)d)'\n",
    "\n",
    "    FORMATS = {\n",
    "        logging.DEBUG: grey + format + reset,\n",
    "        logging.INFO: grey + format + reset,\n",
    "        logging.WARNING: yellow + format + reset,\n",
    "        logging.ERROR: red + format + reset,\n",
    "        logging.CRITICAL: bold_red + format + reset\n",
    "    }\n",
    "\n",
    "    def format(self, record):\n",
    "        log_fmt = self.FORMATS.get(record.levelno)\n",
    "        formatter = logging.Formatter(log_fmt)\n",
    "        return formatter.format(record)\n",
    "    \n",
    "def setup_logger(logger_name, output_dir = None):\n",
    "    # Remove existing handlers if the logger already exists\n",
    "    logger = logging.getLogger(logger_name)\n",
    "    for handler in logger.handlers[:]:\n",
    "        logger.removeHandler(handler)\n",
    "\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    \n",
    "    # create console handler with a higher log level\n",
    "    ch = logging.StreamHandler()\n",
    "    ch.setLevel(logging.DEBUG)\n",
    "    # create formatter and add it to the handlers\n",
    "    formatter = LogCustomFormatter()\n",
    "    ch.setFormatter(formatter)\n",
    "    # add the handlers to logger\n",
    "    logger.addHandler(ch)\n",
    "    \n",
    "    if output_dir:\n",
    "        # create file handler which logs even debug messages\n",
    "        fh = logging.FileHandler(os.path.join(output_dir, 'training_log.log'))\n",
    "        fh.setLevel(logging.DEBUG)\n",
    "        fh.setFormatter(formatter)\n",
    "        logger.addHandler(fh)\n",
    "    return logger\n",
    "\n",
    "def logging_hyperparameters(logger):\n",
    "    logger.info(\"==========Hyperparameters==========\")\n",
    "    logger.info(f\"Device: {DEVICE}\")\n",
    "    # logger.info(f\"Architecture: {ARCH}\")\n",
    "    # logger.info(f\"Encoder: {ENCODER_NAME}\")\n",
    "    # logger.info(f\"Encoder weight: imagenet\")\n",
    "    logger.info(f\"Input size: {INPUT_SIZE}\")\n",
    "    logger.info(f\"Batch size: {BATCH_SIZE}\")\n",
    "    logger.info(f\"Base learning rate: {BASE_LR}\")\n",
    "    logger.info(f\"Max epochs: {MAX_EPOCHS}\")\n",
    "    logger.info(f\"Weight decay: {1e-5}\")\n",
    "    logger.info(\"===================================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_path(model_name):\n",
    "    weight_dir = os.path.join(OUTPUT_DIR, model_name)\n",
    "    os.makedirs(weight_dir, exist_ok=True)\n",
    "    log_dir = weight_dir\n",
    "    logger_name = model_name\n",
    "\n",
    "    return weight_dir, log_dir, logger_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Random Split function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "def split_dataset(csv_path, n_splits = 2, vocab_freq_threshold = 5):\n",
    "    df1 = pd.read_csv(csv_path, delimiter='|')\n",
    "    folder = csv_path.split('/')[-2]\n",
    "    df1['image'] = f'{folder}/' + df1['image']\n",
    "    df2 = pd.read_csv(f'{root_dir}/flickr8k/captions.txt', delimiter=',')\n",
    "    df2['image'] = 'flickr8k/images/' + df2['image']\n",
    "    df = pd.concat([df1, df2], axis=0)\n",
    "    \n",
    "    # df = pd.read_csv(csv_path, delimiter='|')\n",
    "    # folder = csv_path.split('/')[-2]\n",
    "    # df['image'] = f'{folder}/' + df['image']\n",
    "    \n",
    "    df.reset_index(drop=True)\n",
    "\n",
    "    X = df[['image', 'caption']]\n",
    "    y = df['caption']\n",
    "    \n",
    "    vocab = Vocabulary(vocab_freq_threshold)\n",
    "    vocab.build_vocabulary(y.tolist())\n",
    "    \n",
    "    # Initialize StratifiedKFold\n",
    "    stratified_kfold = StratifiedKFold(n_splits=n_splits, shuffle=True)\n",
    "    folds = []\n",
    "    \n",
    "    for train_idx, val_idx in stratified_kfold.split(X, y):\n",
    "        train_set = df.iloc[train_idx]\n",
    "        val_set = df.iloc[val_idx]\n",
    "        folds.append((train_set, val_set))\n",
    "    \n",
    "    return folds, vocab\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, freq_threshold=5):\n",
    "        self.spacy_eng = spacy.load(\"en_core_web_sm\")\n",
    "        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n",
    "        self.stoi = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n",
    "        self.freq_threshold = freq_threshold\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "    \n",
    "    def tokenizer_eng(self, text):\n",
    "        return [tok.text.lower() for tok in self.spacy_eng.tokenizer(text)]\n",
    "\n",
    "    def build_vocabulary(self, sentence_list):\n",
    "        frequencies = {}\n",
    "        idx = 4\n",
    "\n",
    "        for sentence in sentence_list:\n",
    "            for word in self.tokenizer_eng(sentence):\n",
    "                if word not in frequencies:\n",
    "                    frequencies[word] = 1\n",
    "\n",
    "                else:\n",
    "                    frequencies[word] += 1\n",
    "\n",
    "                if frequencies[word] == self.freq_threshold:\n",
    "                    self.stoi[word] = idx\n",
    "                    self.itos[idx] = word\n",
    "                    idx += 1\n",
    "        self._save_vocab_dict()\n",
    "    \n",
    "    def _save_vocab_dict(self):\n",
    "        vocab_dict = {\n",
    "            'itos': self.itos,\n",
    "            'stoi': self.stoi,\n",
    "            'freq_threshold': self.freq_threshold\n",
    "        }\n",
    "\n",
    "        with open(f'{OUTPUT_DIR}/vocab.json', 'w', encoding='utf8', newline='') as f:\n",
    "            json.dump(vocab_dict, f)\n",
    "\n",
    "    def numericalize(self, text):\n",
    "        tokenized_text = self.tokenizer_eng(text)\n",
    "\n",
    "        return [\n",
    "            self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"]\n",
    "            for token in tokenized_text\n",
    "        ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpaceDataset(Dataset):\n",
    "    def __init__(self, root_dir, data_set, vocab, transform=None, freq_threshold=5):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.imgs = data_set['image']\n",
    "        self.captions = data_set['caption']\n",
    "        \n",
    "        self.vocab = vocab\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        caption = self.captions.iloc[idx]\n",
    "        img_path = self.imgs.iloc[idx]\n",
    "\n",
    "        img = Image.open(os.path.join(self.root_dir, img_path)).convert(\"RGB\")\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        numericalized_caption = [self.vocab.stoi[\"<SOS>\"]]\n",
    "        numericalized_caption += self.vocab.numericalize(caption)\n",
    "        numericalized_caption.append(self.vocab.stoi[\"<EOS>\"])\n",
    "\n",
    "        return img, torch.tensor(numericalized_caption)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCollate:\n",
    "    def __init__(self, pad_idx):\n",
    "        self.pad_idx = pad_idx\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        imgs = [item[0].unsqueeze(0) for item in batch]\n",
    "        imgs = torch.cat(imgs, dim=0)\n",
    "        targets = [item[1] for item in batch]\n",
    "        targets = pad_sequence(targets, batch_first=False, padding_value=self.pad_idx)\n",
    "\n",
    "        return imgs, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve Loader from Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader(\n",
    "    root_folder,\n",
    "    fold,\n",
    "    vocab,\n",
    "    transform,\n",
    "    batch_size=32,\n",
    "    num_workers=8,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "):\n",
    "    train_images, val_images = fold\n",
    "    train_set = SpaceDataset(root_folder, train_images, vocab, transform=transform)\n",
    "    val_set = SpaceDataset(root_folder, val_images, vocab, transform=transform)\n",
    "\n",
    "    pad_idx = vocab.stoi[\"<PAD>\"]\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        dataset=train_set,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        shuffle=shuffle,\n",
    "        pin_memory=pin_memory,\n",
    "        collate_fn=MyCollate(pad_idx=pad_idx),\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        dataset=train_set,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        shuffle=False,\n",
    "        pin_memory=pin_memory,\n",
    "        collate_fn=MyCollate(pad_idx=pad_idx),\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, embed_size, train_CNN = False):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        self.train_CNN = train_CNN\n",
    "        self.inception = models.inception_v3(pretrained=True, aux_logits=True)\n",
    "        self.inception.fc = nn.Linear(self.inception.fc.in_features, embed_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.times = []\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, images):\n",
    "        features = self.inception(images)\n",
    "        \n",
    "        return self.dropout(self.relu(features[0]))\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, self.hidden_size, self.num_layers)\n",
    "        self.linear = nn.Linear(self.hidden_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, features, captions):\n",
    "        features = features.unsqueeze(0)\n",
    "  \n",
    "        embeddings = self.dropout(self.embed(captions))\n",
    "\n",
    "        embeddings = torch.cat((features, embeddings), dim=0)\n",
    "        hiddens, _ = self.lstm(embeddings)\n",
    "        outputs = self.linear(hiddens)\n",
    "        return outputs\n",
    "\n",
    "class CNNtoRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers = 1):\n",
    "        super(CNNtoRNN, self).__init__()\n",
    "        self.encoderCNN = EncoderCNN(embed_size)\n",
    "        self.decoderRNN = DecoderRNN(embed_size, hidden_size, vocab_size, num_layers)\n",
    "    \n",
    "    def forward(self, images, captions):\n",
    "        features = self.encoderCNN(images)\n",
    "        outputs = self.decoderRNN(features, captions)\n",
    "        return outputs\n",
    "    \n",
    "    def caption_image(self, image, vocabulary, max_length=50):\n",
    "        result_caption = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            x = self.encoderCNN(image).unsqueeze(0)\n",
    "            states = None\n",
    "            \n",
    "            for _ in range(max_length):                \n",
    "                hiddens, states = self.decoderRNN.lstm(x, states)\n",
    "                output = self.decoderRNN.linear(hiddens.squeeze(0))\n",
    "\n",
    "                predicted = output.argmax()\n",
    "                \n",
    "                result_caption.append(predicted.item())\n",
    "                x = self.decoderRNN.embed(predicted).unsqueeze(0)\n",
    "                \n",
    "                if vocabulary.itos[predicted.item()] == '<EOS>':\n",
    "                    break\n",
    "                \n",
    "        return [vocabulary.itos[idx] for idx in result_caption][1:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics:\n",
    "    def __init__(self):\n",
    "        self.global_step = 0\n",
    "        self.train_loss = AverageMeter()\n",
    "        self.val_loss = AverageMeter()\n",
    "        \n",
    "        self.bleu_score = 0\n",
    "        self.references = []\n",
    "        self.hypotheses = []\n",
    "        \n",
    "    def reset(self):\n",
    "        self.train_loss.reset()\n",
    "        self.val_loss.reset()\n",
    "        self.bleu_score = 0\n",
    "        self.references = []\n",
    "        self.hypotheses = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement Train Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "class TrainHelper:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        vocab: Vocabulary,\n",
    "        train_loader: DataLoader,\n",
    "        val_loader: DataLoader,\n",
    "        optimizer: torch.optim,\n",
    "        metrics: Metrics,\n",
    "        # tensorboard_writer = None,\n",
    "        current_epoch: int = 1,\n",
    "        epochs: int = 1,\n",
    "        current_fold: int = 1,\n",
    "        folds: int = 1,\n",
    "        loss_criteria: torch.nn = None\n",
    "    ):\n",
    "        self.model = model.to(DEVICE)\n",
    "        self.vocab = vocab\n",
    "        # self.tensorboard_writer = tensorboard_writer\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.optimizer = optimizer\n",
    "        self.metrics = metrics\n",
    "        self.current_epoch = current_epoch\n",
    "        self.epochs = epochs\n",
    "        self.current_fold = current_fold\n",
    "        self.folds = folds\n",
    "        self.loss_criteria = loss_criteria\n",
    "    \n",
    "    def train(self):\n",
    "        self.model.train()\n",
    "        length = len(self.train_loader)\n",
    "        self.metrics.reset()\n",
    "        \n",
    "        with tqdm(self.train_loader, total=length, unit=\"batch\") as tepoch:\n",
    "            for batch, (image, caption) in enumerate(tepoch):\n",
    "                tepoch.set_description(f'Fold[{self.current_fold}/{self.folds}] Epoch[{self.current_epoch}/{self.epochs}] Train')\n",
    "                \n",
    "                n = image.shape[0]\n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                image = image.to(DEVICE)\n",
    "                caption = caption.to(DEVICE)\n",
    "                \n",
    "                outputs = self.model(image, caption[:-1])\n",
    "                loss = self.loss_criteria(outputs.reshape(-1, outputs.shape[2]), caption.reshape(-1))\n",
    "                \n",
    "                # self.tensorboard_writer.add_scalar(\"Training loss\", loss.item(), global_step = self.metrics.global_step)\n",
    "                loss.backward()\n",
    "                \n",
    "                self.metrics.train_loss.update(loss.item(), n)\n",
    "                \n",
    "                self.metrics.global_step += 1\n",
    "                \n",
    "                self.optimizer.step()\n",
    "                free_gpu_cache(image, caption)\n",
    "                free_cuda_cache()\n",
    "\n",
    "                string = self._train_log()\n",
    "                tepoch.set_postfix_str(string)\n",
    "        return self.metrics\n",
    "    \n",
    "    def eval(self):\n",
    "        self.model.eval()\n",
    "        length = len(self.val_loader)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            with tqdm(self.val_loader, total=length, unit=\"batch\") as tepoch:\n",
    "                for (images, captions) in tepoch:\n",
    "                    tepoch.set_description(f'Fold[{self.current_fold}/{self.folds}] Epoch[{self.current_epoch}/{self.epochs}] Eval')\n",
    "                    n = images.shape[0]\n",
    "                    \n",
    "                    images = images.to(DEVICE)\n",
    "                    captions = captions.to(DEVICE)                  \n",
    " \n",
    "                    for image, caption in zip(images, captions):\n",
    "                        # Generate caption for each image\n",
    "                        predicted_caption = model.caption_image(image.unsqueeze(0), self.vocab)\n",
    "\n",
    "                        # Convert reference to words\n",
    "                        reference = [self.vocab.itos[idx.item()] for idx in caption if idx.item() != 0]\n",
    "                        reference = [' '.join(reference)]\n",
    "                        hypothese = ' '.join(predicted_caption)\n",
    "\n",
    "                        self.metrics.references.append(reference)\n",
    "                        self.metrics.hypotheses.append(hypothese)\n",
    "                    \n",
    "                    self.metrics.bleu_score = corpus_bleu(self.metrics.references, self.metrics.hypotheses)  \n",
    "                        \n",
    "                    free_gpu_cache(images, captions)\n",
    "                    free_cuda_cache()\n",
    "                    string = self._val_log()\n",
    "                    tepoch.set_postfix_str(string)\n",
    "        return self.metrics\n",
    "    \n",
    "    def _train_log(self):\n",
    "        return f\"Loss: {self.metrics.train_loss.avg:.3f}\"\n",
    "\n",
    "    def _val_log(self):\n",
    "        return f\"BLEU Score: {self.metrics.bleu_score:.3f}\"\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        folds,\n",
    "        vocab,\n",
    "        epochs,\n",
    "        model: nn.Module,\n",
    "        optimizer: torch.optim,\n",
    "        loss_criteria: torch.nn = None\n",
    "    ):\n",
    "        # torch.backends.cudnn.benchmark = True\n",
    "        # for tensorboard\n",
    "        self.folds = folds\n",
    "        self.vocab = vocab\n",
    "        self.epochs = epochs\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_criteria = loss_criteria\n",
    "        \n",
    "        self.weight_dir, log_dir, logger_name = init_path(model.__class__.__name__)\n",
    "        self.logger = setup_logger(logger_name, log_dir)\n",
    "        self.stale = 0\n",
    "        self.best_overall = 0\n",
    "        self.start_time = datetime.datetime.now()\n",
    "    \n",
    "    def run(self):\n",
    "        logging_hyperparameters(self.logger)\n",
    "        transform = T.Compose(\n",
    "            [\n",
    "                T.Resize(INPUT_SIZE),\n",
    "                T.ToTensor(),\n",
    "                T.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        \n",
    "        for idx, fold in enumerate(self.folds):\n",
    "            wait_until_enough_gpu_memory()\n",
    "\n",
    "            # train_loader, val_loader = get_loader(FOLDER_PATH, fold, self.vocab, transform, \n",
    "            #                                       batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=True, pin_memory=True)\n",
    "\n",
    "            train_loader, val_loader = get_loader(root_dir, fold, self.vocab, transform, \n",
    "                                                  batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=True, pin_memory=True)\n",
    "\n",
    "\n",
    "            self._loops(idx, train_loader, val_loader)\n",
    "            \n",
    "            free_gpu_cache(train_loader, val_loader)\n",
    "    \n",
    "    def _loops(self, idx, train_loader, val_loader):\n",
    "        N_FOLDS = len(self.folds)\n",
    "        metrics = Metrics()\n",
    "        \n",
    "        if CHECKPOINT is not None:\n",
    "            if os.path.exists(CHECKPOINT):\n",
    "                checkpoint = torch.load(CHECKPOINT)\n",
    "                self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "                self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "                start_epoch = checkpoint['epoch']\n",
    "                self.best_overall = checkpoint['best_f1']\n",
    "                self.logger.info(f\"Resume training from epoch {start_epoch}\")\n",
    "            else:\n",
    "                self.logger.info(f\"Checkpoint not found, start training from epoch 1\")\n",
    "        \n",
    "        scheduler = optim.lr_scheduler.CyclicLR(self.optimizer, base_lr=BASE_LR, max_lr=MAX_LR, cycle_momentum=False)\n",
    "        \n",
    "        for epoch in range(1, self.epochs + 1):\n",
    "            trainer_helper = TrainHelper(self.model, self.vocab, train_loader, val_loader, self.optimizer, metrics, epoch, self.epochs, idx + 1, N_FOLDS, self.loss_criteria)\n",
    "\n",
    "            trainer_helper.train()\n",
    "\n",
    "            trainer_helper.eval()\n",
    "\n",
    "            self._save_best(idx, epoch, metrics)\n",
    "            \n",
    "            scheduler.step()\n",
    "            \n",
    "            if self._is_early_stop():\n",
    "                break\n",
    "    \n",
    "    def _save_best(self, fold, epoch, metrics):\n",
    "        #Save best model\n",
    "        to_save = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'best_f1': self.best_overall,\n",
    "        }\n",
    "        file_name = self._best_model_file_name(fold, epoch)\n",
    "        if metrics.bleu_score > self.best_overall:\n",
    "            self.logger.info(f\"Best model found at fold {fold + 1} - epoch {epoch}, saving model\")\n",
    "            self.best_overall = metrics.bleu_score\n",
    "            torch.save(to_save, os.path.join(self.weight_dir, file_name)) # only save best to prevent output memory exceed error\n",
    "\n",
    "            self.stale = 0\n",
    "        else:\n",
    "            self.stale += 1  \n",
    "                \n",
    "        if epoch % SAVE_INTERVAL == 0 or epoch == self.epochs:\n",
    "            self.logger.info(f\"Save model at fold {fold + 1} - epoch {epoch}, saving model\")\n",
    "            torch.save(to_save, os.path.join(self.weight_dir, file_name))\n",
    "            \n",
    "    def _best_model_file_name(self, fold, epoch):\n",
    "        end_time = datetime.datetime.now()\n",
    "        name = f\"{self.start_time}_{end_time}_fold-{fold + 1}_epoch-{epoch}_{INPUT_SIZE[0]}_BS-{BATCH_SIZE}_bleu-score-{self.best_overall:.3f}.pth\"\n",
    "        return name\n",
    "    \n",
    "    def _is_early_stop(self) -> bool:\n",
    "        if self.stale > EARLY_STOP_THRESHOLD:\n",
    "            self.logger.info(f\"No improvement {EARLY_STOP_THRESHOLD} consecutive epochs, early stopping\")\n",
    "            return True\n",
    "        return False\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jiggle/anaconda3/envs/mle-project-1/lib/python3.9/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=2.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "folds, vocab = split_dataset(CAPTIONS_DIR, N_SPLITS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jiggle/anaconda3/envs/mle-project-1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/jiggle/anaconda3/envs/mle-project-1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "\u001b[0;37m2024-02-08 09:26:50,851 - CNNtoRNN - INFO - ==========Hyperparameters========== (1707100198.py:56)\u001b[0m\n",
      "\u001b[0;37m2024-02-08 09:26:50,852 - CNNtoRNN - INFO - Device: cuda (1707100198.py:57)\u001b[0m\n",
      "\u001b[0;37m2024-02-08 09:26:50,853 - CNNtoRNN - INFO - Input size: (384, 384) (1707100198.py:61)\u001b[0m\n",
      "\u001b[0;37m2024-02-08 09:26:50,853 - CNNtoRNN - INFO - Batch size: 16 (1707100198.py:62)\u001b[0m\n",
      "\u001b[0;37m2024-02-08 09:26:50,854 - CNNtoRNN - INFO - Base learning rate: 0.1 (1707100198.py:63)\u001b[0m\n",
      "\u001b[0;37m2024-02-08 09:26:50,854 - CNNtoRNN - INFO - Max epochs: 200 (1707100198.py:64)\u001b[0m\n",
      "\u001b[0;37m2024-02-08 09:26:50,855 - CNNtoRNN - INFO - Weight decay: 1e-05 (1707100198.py:65)\u001b[0m\n",
      "\u001b[0;37m2024-02-08 09:26:50,856 - CNNtoRNN - INFO - =================================== (1707100198.py:66)\u001b[0m\n",
      "Fold[1/2] Epoch[1/200] Train:   0%|          | 0/1289 [00:00<?, ?batch/s]/home/jiggle/anaconda3/envs/mle-project-1/lib/python3.9/site-packages/torch/nn/modules/conv.py:456: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n",
      "Fold[1/2] Epoch[1/200] Train: 100%|██████████| 1289/1289 [12:18<00:00,  1.75batch/s, Loss: 5.197]\n",
      "Fold[1/2] Epoch[1/200] Eval: 100%|██████████| 1289/1289 [42:03<00:00,  1.96s/batch, BLEU Score: 0.025]\n",
      "\u001b[0;37m2024-02-08 10:21:13,306 - CNNtoRNN - INFO - Best model found at fold 1 - epoch 1, saving model (330939360.py:92)\u001b[0m\n",
      "Fold[1/2] Epoch[2/200] Train: 100%|██████████| 1289/1289 [12:30<00:00,  1.72batch/s, Loss: 5.098]\n",
      "Fold[1/2] Epoch[2/200] Eval: 100%|██████████| 1289/1289 [46:36<00:00,  2.17s/batch, BLEU Score: 0.062]\n",
      "\u001b[0;37m2024-02-08 11:20:20,158 - CNNtoRNN - INFO - Best model found at fold 1 - epoch 2, saving model (330939360.py:92)\u001b[0m\n",
      "Fold[1/2] Epoch[3/200] Train: 100%|██████████| 1289/1289 [12:52<00:00,  1.67batch/s, Loss: 5.219]\n",
      "Fold[1/2] Epoch[3/200] Eval:  61%|██████    | 788/1289 [28:19<29:18,  3.51s/batch, BLEU Score: 0.033]"
     ]
    }
   ],
   "source": [
    "model = CNNtoRNN(EMBED_SIZE, HIDDEN_SIZE, len(vocab), num_layers = 1).to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab.stoi[\"<PAD>\"])\n",
    "optimizer = optim.Adam(model.parameters(), lr=BASE_LR)\n",
    "\n",
    "# Only finetune the CNN\n",
    "for name, param in model.encoderCNN.inception.named_parameters():\n",
    "    if \"fc.weight\" in name or \"fc.bias\" in name:\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False\n",
    "        \n",
    "train = Trainer(folds, vocab, MAX_EPOCHS, model, optimizer, criterion)\n",
    "train.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mle-project-1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
